# RAG System for Files Management ğŸ“ğŸ¤–

Un sistema avanzato di Retrieval-Augmented Generation (RAG) per l'analisi e gestione intelligente dei file del tuo sistema, con integrazione AI per chat conversazionale.

## âœ¨ FunzionalitÃ  Principali

### ğŸ“Š Analisi File Avanzata
- **Scansione ricorsiva** di directory con metadati dettagliati
- **Analisi contenuto** per diversi tipi di file (testo, immagini, PDF, Excel, etc.)
- **Calcolo dimensioni** con visualizzazione human-readable
- **Rilevamento tipo MIME** e categorizzazione automatica
- **Suggerimenti pulizia** per file temporanei, cache e duplicati

### ğŸ” Sistema RAG con Vector Search
- **Embeddings semantici** con Sentence Transformers (all-MiniLM-L6-v2)
- **Database vettoriale ChromaDB** per ricerca veloce e accurata
- **Ricerca per similaritÃ ** con ranking di rilevanza
- **Filtri avanzati** per dimensione, tipo, estensione
- **Contesto persistente** per l'ultima cartella scansionata

### ğŸ’¬ Chat AI Conversazionale
- **Integrazione LM Studio** per usare qualsiasi modello LLM locale
- **Risposte contestuali** basate sui file scansionati
- **Analisi intelligente** con suggerimenti personalizzati
- **CompatibilitÃ  estesa** con modelli Mistral, Llama, Qwen, ecc.
- **Supporto multilingua** (italiano/inglese)

### ğŸ“ˆ Visualizzazioni Dati
- **Grafico vettori 3D** con PCA per visualizzare la distribuzione dei file
- **Categorizzazione visuale** per tipo di file con colori distinti
- **Distribuzione tipi file** con grafici interattivi

## ğŸš€ Quick Start

### Prerequisiti
- Docker e Docker Compose
- **LM Studio** installato e configurato
- Oppure Python 3.11+ per esecuzione locale

### Installazione con Docker (Consigliato)

```bash
# Clona il repository
git clone https://github.com/MarcoSalmaso/RAG-System-for-Files-Management.git
cd RAG-System-for-Files-Management

# Avvia il sistema
docker-compose up --build

# Apri nel browser
# Frontend: http://localhost:5173
# Backend API: http://localhost:8000
```

### Configurazione LM Studio

1. **Scarica e installa LM Studio**: https://lmstudio.ai
2. **Carica un modello** (es. Mistral 7B, Llama 3.2, Qwen):
   - Vai nella tab "Models" 
   - Cerca e scarica un modello compatibile
3. **Avvia il server locale**:
   - Vai nella tab "Local Server"
   - Carica il modello nella chat
   - Clicca "Start Server" su porta 1234
   - âœ… Verifica che appaia "Server running on http://localhost:1234"

### Installazione Locale

```bash
# Backend
cd backend
pip install -r requirements.txt
python main.py

# Frontend (in un'altra finestra)
cd frontend-simple
python -m http.server 5173
```

## ğŸ“– Utilizzo

### 1. Scansiona una Directory
- Vai alla tab **"Scansiona"**
- Inserisci il percorso (es. `~/Desktop` o `/Users/nome/Documents`)
- Clicca su "Analizza Directory"
- Il sistema indicizzerÃ  tutti i file trovati

### 2. Chat Intelligente con LM Studio
- **Assicurati che LM Studio sia attivo** su localhost:1234
- Vai alla tab **"Chat AI"**
- Fai domande sui tuoi file:
  - "Quali sono i file piÃ¹ grandi?"
  - "Mostrami le cartelle piÃ¹ pesanti"
  - "Cosa posso eliminare per liberare spazio?"
  - "Dammi un riassunto della cartella"
  - "Trova tutti i file immagine"
- Il tuo modello LM Studio risponderÃ  con contesto completo sui file!

### 3. Visualizza Grafico Vettori
- Vai alla tab **"Grafico Vettori"**
- Esplora la rappresentazione 3D dei tuoi file
- I colori indicano diverse categorie (immagini, documenti, codice, etc.)
- Passa il mouse sui punti per vedere i dettagli

## ğŸ—ï¸ Architettura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚â”€â”€â”€â”€â–¶â”‚   Backend API   â”‚â”€â”€â”€â”€â–¶â”‚   ChromaDB      â”‚
â”‚   (HTML/JS)     â”‚     â”‚   (FastAPI)     â”‚     â”‚   (Vectors)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   AI Models         â”‚
                    â”‚ - LM Studio API     â”‚
                    â”‚ - Sentence-Transformâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Stack Tecnologico

### Backend
- **FastAPI**: Framework web moderno e veloce
- **ChromaDB**: Database vettoriale per embeddings
- **Sentence Transformers**: Generazione embeddings semantici
- **LM Studio Integration**: API REST per modelli LLM locali
- **HTTPX**: Client HTTP asincrono per LM Studio API
- **Python-Magic**: Rilevamento tipo MIME
- **PyPDF2, python-docx, openpyxl**: Parsing documenti
- **Pillow**: Analisi immagini

### Frontend
- **HTML5/CSS3/JavaScript**: Interfaccia responsive
- **Tailwind CSS**: Styling moderno
- **Chart.js**: Visualizzazione grafici
- **Three.js**: Rendering 3D vettori
- **Fetch API**: Comunicazione con backend

## ğŸ“‹ API Endpoints

| Endpoint | Metodo | Descrizione |
|----------|--------|-------------|
| `/scan` | POST | Scansiona una directory |
| `/chat` | POST | Chat conversazionale AI |
| `/query` | POST | Query RAG sui file |
| `/vectors` | GET | Vettori per visualizzazione |
| `/file-types` | GET | Distribuzione tipi file |
| `/clear` | DELETE | Pulisce l'indice |

## ğŸ”§ Configurazione

### Docker Compose
```yaml
services:
  backend:
    ports:
      - "8000:8000"
    volumes:
      - ./backend/chroma_db:/app/chroma_db  # Persistenza DB
      - /:/host_root:ro  # Accesso file sistema (read-only)

  frontend:
    ports:
      - "5173:80"
```

### Variabili Ambiente
- `CHROMA_DB_PATH`: Path database vettoriale (default: `./chroma_db`)
- `MODEL_CACHE_DIR`: Cache modelli AI (default: `~/.cache/huggingface`)
- `MAX_FILE_SIZE`: Dimensione max file analizzabile (default: 100MB)

## ğŸ“ Struttura Progetto

```
RAG-System-for-Files-Management/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # API FastAPI
â”‚   â”œâ”€â”€ rag_system.py        # Core RAG + AI
â”‚   â”œâ”€â”€ file_analyzer.py     # Analisi file
â”‚   â”œâ”€â”€ requirements.txt     # Dipendenze Python
â”‚   â””â”€â”€ Dockerfile           
â”œâ”€â”€ frontend-simple/
â”‚   â”œâ”€â”€ index.html           # Interfaccia web
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml       # Orchestrazione servizi
â””â”€â”€ README.md               # Questo file
```

## ğŸš¦ Limitazioni e Note

- **LM Studio richiesto**: Necessario avere LM Studio installato e attivo
- **CompatibilitÃ  modelli**: Testato con Mistral, Llama, Qwen (modelli senza ruolo "system")
- **Memoria**: RAM richiesta dipende dal modello caricato in LM Studio
- **File grandi**: File >100MB potrebbero rallentare l'indicizzazione
- **Percorsi Docker**: In Docker, i percorsi locali sono mappati su `/host_root`
- **ConnettivitÃ **: Il container Docker deve poter accedere a `host.docker.internal:1234`

## ğŸ“ License

MIT License - Vedi file [LICENSE](LICENSE) per dettagli

## ğŸ™ Acknowledgments

- [ChromaDB](https://www.trychroma.com/) per il database vettoriale
- [LM Studio](https://lmstudio.ai/) per l'interfaccia AI locale
- [Hugging Face](https://huggingface.co/) per i modelli AI
- [Sentence Transformers](https://www.sbert.net/) per gli embeddings

---

Sviluppato con â¤ï¸ per rendere la gestione file piÃ¹ intelligente e conversazionale.